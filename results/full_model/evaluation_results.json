{
  "overall_metrics": {
    "accuracy": 0.26944684528954194,
    "precision": 0.27061844663349854,
    "recall": 0.25615090104439847,
    "f1_macro": 0.21313314747324147,
    "f1_weighted": 0.2214208176460708
  },
  "subject_metrics": {
    "": {
      "accuracy": 0.2667233276672333,
      "f1": 0.19865922380131704,
      "n_samples": 20002
    },
    "abstract_algebra": {
      "accuracy": 0.16,
      "f1": 0.09821428571428571,
      "n_samples": 25
    },
    "anatomy": {
      "accuracy": 0.2857142857142857,
      "f1": 0.18253968253968253,
      "n_samples": 21
    },
    "astronomy": {
      "accuracy": 0.391304347826087,
      "f1": 0.20833333333333331,
      "n_samples": 23
    },
    "business_ethics": {
      "accuracy": 0.15384615384615385,
      "f1": 0.08,
      "n_samples": 26
    },
    "clinical_knowledge": {
      "accuracy": 0.2,
      "f1": 0.13002364066193853,
      "n_samples": 55
    },
    "college_biology": {
      "accuracy": 0.23684210526315788,
      "f1": 0.15092879256965944,
      "n_samples": 38
    },
    "college_chemistry": {
      "accuracy": 0.25,
      "f1": 0.21195652173913043,
      "n_samples": 20
    },
    "college_computer_science": {
      "accuracy": 0.25,
      "f1": 0.1111111111111111,
      "n_samples": 24
    },
    "college_mathematics": {
      "accuracy": 0.14285714285714285,
      "f1": 0.10512820512820513,
      "n_samples": 21
    },
    "college_medicine": {
      "accuracy": 0.2708333333333333,
      "f1": 0.11206896551724138,
      "n_samples": 48
    },
    "college_physics": {
      "accuracy": 0.3076923076923077,
      "f1": 0.1880952380952381,
      "n_samples": 26
    },
    "computer_security": {
      "accuracy": 0.14285714285714285,
      "f1": 0.07894736842105263,
      "n_samples": 21
    },
    "conceptual_physics": {
      "accuracy": 0.3076923076923077,
      "f1": 0.19886363636363635,
      "n_samples": 52
    },
    "econometrics": {
      "accuracy": 0.2692307692307692,
      "f1": 0.21666666666666667,
      "n_samples": 26
    },
    "electrical_engineering": {
      "accuracy": 0.4444444444444444,
      "f1": 0.3472222222222222,
      "n_samples": 27
    },
    "elementary_mathematics": {
      "accuracy": 0.2857142857142857,
      "f1": 0.24970238095238095,
      "n_samples": 70
    },
    "formal_logic": {
      "accuracy": 0.43333333333333335,
      "f1": 0.1511627906976744,
      "n_samples": 30
    },
    "global_facts": {
      "accuracy": 0.16,
      "f1": 0.10166666666666666,
      "n_samples": 25
    },
    "high_school_biology": {
      "accuracy": 0.26865671641791045,
      "f1": 0.16396713615023473,
      "n_samples": 67
    },
    "high_school_chemistry": {
      "accuracy": 0.20754716981132076,
      "f1": 0.18033610533610533,
      "n_samples": 53
    },
    "high_school_computer_science": {
      "accuracy": 0.19047619047619047,
      "f1": 0.08333333333333333,
      "n_samples": 21
    },
    "high_school_european_history": {
      "accuracy": 0.23684210526315788,
      "f1": 0.14305555555555555,
      "n_samples": 38
    },
    "high_school_geography": {
      "accuracy": 0.375,
      "f1": 0.13953488372093023,
      "n_samples": 32
    },
    "high_school_government_and_politics": {
      "accuracy": 0.391304347826087,
      "f1": 0.140625,
      "n_samples": 46
    },
    "high_school_macroeconomics": {
      "accuracy": 0.3372093023255814,
      "f1": 0.12608695652173912,
      "n_samples": 86
    },
    "high_school_mathematics": {
      "accuracy": 0.3584905660377358,
      "f1": 0.291683569979716,
      "n_samples": 53
    },
    "high_school_microeconomics": {
      "accuracy": 0.33962264150943394,
      "f1": 0.1267605633802817,
      "n_samples": 53
    },
    "high_school_physics": {
      "accuracy": 0.2647058823529412,
      "f1": 0.10714285714285714,
      "n_samples": 34
    },
    "high_school_psychology": {
      "accuracy": 0.4186046511627907,
      "f1": 0.1861888111888112,
      "n_samples": 129
    },
    "high_school_statistics": {
      "accuracy": 0.4489795918367347,
      "f1": 0.15492957746478872,
      "n_samples": 49
    },
    "high_school_us_history": {
      "accuracy": 0.2037037037037037,
      "f1": 0.15937323546019197,
      "n_samples": 54
    },
    "high_school_world_history": {
      "accuracy": 0.2692307692307692,
      "f1": 0.21689231248054774,
      "n_samples": 52
    },
    "human_aging": {
      "accuracy": 0.3783783783783784,
      "f1": 0.14,
      "n_samples": 37
    },
    "human_sexuality": {
      "accuracy": 0.12,
      "f1": 0.06,
      "n_samples": 25
    },
    "international_law": {
      "accuracy": 0.38095238095238093,
      "f1": 0.14814814814814814,
      "n_samples": 21
    },
    "jurisprudence": {
      "accuracy": 0.23809523809523808,
      "f1": 0.19886363636363635,
      "n_samples": 21
    },
    "logical_fallacies": {
      "accuracy": 0.23809523809523808,
      "f1": 0.16227180527383367,
      "n_samples": 42
    },
    "machine_learning": {
      "accuracy": 0.2222222222222222,
      "f1": 0.23990683229813664,
      "n_samples": 27
    },
    "management": {
      "accuracy": 0.23809523809523808,
      "f1": 0.1,
      "n_samples": 21
    },
    "marketing": {
      "accuracy": 0.2549019607843137,
      "f1": 0.2345192915876028,
      "n_samples": 51
    },
    "medical_genetics": {
      "accuracy": 0.2962962962962963,
      "f1": 0.11764705882352941,
      "n_samples": 27
    },
    "miscellaneous": {
      "accuracy": 0.22818791946308725,
      "f1": 0.19047619047619047,
      "n_samples": 149
    },
    "moral_disputes": {
      "accuracy": 0.2911392405063291,
      "f1": 0.23052464228934816,
      "n_samples": 79
    },
    "moral_scenarios": {
      "accuracy": 0.25961538461538464,
      "f1": 0.10305343511450382,
      "n_samples": 208
    },
    "nutrition": {
      "accuracy": 0.21428571428571427,
      "f1": 0.08823529411764706,
      "n_samples": 56
    },
    "philosophy": {
      "accuracy": 0.2835820895522388,
      "f1": 0.21,
      "n_samples": 67
    },
    "prehistory": {
      "accuracy": 0.32926829268292684,
      "f1": 0.16302588996763753,
      "n_samples": 82
    },
    "professional_accounting": {
      "accuracy": 0.328125,
      "f1": 0.2064102564102564,
      "n_samples": 64
    },
    "professional_law": {
      "accuracy": 0.2785923753665689,
      "f1": 0.15835779850607604,
      "n_samples": 341
    },
    "professional_medicine": {
      "accuracy": 0.4714285714285714,
      "f1": 0.16019417475728157,
      "n_samples": 70
    },
    "professional_psychology": {
      "accuracy": 0.2532467532467532,
      "f1": 0.19055851681531027,
      "n_samples": 154
    },
    "public_relations": {
      "accuracy": 0.3235294117647059,
      "f1": 0.2146153846153846,
      "n_samples": 34
    },
    "security_studies": {
      "accuracy": 0.3333333333333333,
      "f1": 0.125,
      "n_samples": 57
    },
    "sociology": {
      "accuracy": 0.22641509433962265,
      "f1": 0.15308747855917665,
      "n_samples": 53
    },
    "us_foreign_policy": {
      "accuracy": 0.13043478260869565,
      "f1": 0.0625,
      "n_samples": 23
    },
    "virology": {
      "accuracy": 0.21428571428571427,
      "f1": 0.12774725274725274,
      "n_samples": 28
    },
    "world_religions": {
      "accuracy": 0.3392857142857143,
      "f1": 0.24131944444444442,
      "n_samples": 56
    }
  },
  "confidence_metrics": {
    "high_confidence_ratio": 0.0,
    "high_confidence_accuracy": 0.0,
    "low_confidence_accuracy": 0.26944684528954194
  }
}